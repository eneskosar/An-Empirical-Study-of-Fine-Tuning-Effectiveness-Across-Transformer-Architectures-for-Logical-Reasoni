{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eneskosar/paper1/blob/main/2decoders_ft.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "386b3f9f",
      "metadata": {
        "id": "386b3f9f"
      },
      "source": [
        "# decoder15_multi — Restricted-logit classification w/ LoRA + early stopping\n",
        "\n",
        "This notebook fine-tunes multiple ≤2B decoder LMs on FOLIO (3-way entailment) using restricted logits (A/B/C tokens), with LoRA for efficiency and early stopping.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27be77d9",
      "metadata": {
        "id": "27be77d9"
      },
      "outputs": [],
      "source": [
        "# CELL 0 — Mount Drive (for saving adapters)\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2849a7a3",
      "metadata": {
        "id": "2849a7a3"
      },
      "outputs": [],
      "source": [
        "# CELL 1 — Install dependencies\n",
        "!pip -q install -U transformers==4.46.0 datasets==2.21.0 accelerate==0.34.2 peft==0.12.0 sentencepiece safetensors evaluate\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a9130151",
      "metadata": {
        "id": "a9130151"
      },
      "outputs": [],
      "source": [
        "# CELL 2 — (Optional) Hugging Face login (needed for gated models/datasets)\n",
        "from huggingface_hub import login\n",
        "login()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a13ef33",
      "metadata": {
        "id": "7a13ef33"
      },
      "outputs": [],
      "source": [
        "# CELL 3 — Config (models, LoRA, early stopping)\n",
        "import os, re, torch, random, numpy as np\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "\n",
        "# ---- output root ----\n",
        "OUT_ROOT = \"/content/drive/MyDrive/logic/decoder15_multi_lora\"\n",
        "os.makedirs(OUT_ROOT, exist_ok=True)\n",
        "\n",
        "# ---- model list (≤2B) ----\n",
        "MODEL_LIST = [\n",
        "    \"google/gemma-2-2b-it\",\n",
        "    \"stabilityai/stablelm-2-zephyr-1_6b\",\n",
        "    \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
        "    \"Qwen/Qwen2.5-1.5B-Instruct\",\n",
        "    \"meta-llama/Llama-3.2-1B-Instruct\",\n",
        "]\n",
        "\n",
        "# ---- data ----\n",
        "DATASET_NAME = \"yale-nlp/FOLIO\"\n",
        "\n",
        "# ---- training ----\n",
        "MAX_LEN      = 1024\n",
        "EPOCHS       = 30\n",
        "LR           = 5e-5\n",
        "WEIGHT_DECAY = 1e-3\n",
        "WARMUP_RATIO = 0.03\n",
        "\n",
        "TRAIN_BS     = 1\n",
        "EVAL_BS      = 2\n",
        "GRAD_ACCUM   = 16\n",
        "\n",
        "LOG_STEPS    = 20\n",
        "EVAL_STEPS   = 200\n",
        "SAVE_STEPS   = 200\n",
        "SEED         = 42\n",
        "\n",
        "# ---- early stopping ----\n",
        "EARLY_STOP_PATIENCE = 3  # stop after N evals without improvement\n",
        "EARLY_STOP_THRESHOLD = 0.0\n",
        "\n",
        "# ---- LoRA (PEFT) ----\n",
        "LORA_R = 16\n",
        "LORA_ALPHA = 32\n",
        "LORA_DROPOUT = 0.05\n",
        "\n",
        "def seed_all(seed=SEED):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "seed_all(SEED)\n",
        "print(\"OUT_ROOT:\", OUT_ROOT)\n",
        "print(\"Models:\", MODEL_LIST)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bda74bd6",
      "metadata": {
        "id": "bda74bd6"
      },
      "outputs": [],
      "source": [
        "# CELL 4 — Load dataset\n",
        "from datasets import load_dataset\n",
        "ds = load_dataset(DATASET_NAME)\n",
        "print(ds)\n",
        "print(\"Columns:\", ds[\"train\"].column_names)\n",
        "print(\"Example:\", ds[\"train\"][0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fec298e8",
      "metadata": {
        "id": "fec298e8"
      },
      "outputs": [],
      "source": [
        "# CELL 5 — Build prompts + labels (3-way: A/B/C)\n",
        "from datasets import DatasetDict\n",
        "from collections import Counter\n",
        "\n",
        "LABEL_TO_LETTER = {\"True\":\"A\", \"False\":\"B\", \"Unknown\":\"C\"}\n",
        "ALT_LABELS = {\n",
        "    \"Uncertain\":\"Unknown\", \"uncertain\":\"Unknown\",\n",
        "    \"true\":\"True\", \"false\":\"False\", \"unknown\":\"Unknown\"\n",
        "}\n",
        "\n",
        "def normalize_label(lbl: str) -> str:\n",
        "    s = str(lbl).strip()\n",
        "    s = ALT_LABELS.get(s, s)\n",
        "    if s not in LABEL_TO_LETTER:\n",
        "        raise ValueError(f\"Unexpected label: {lbl!r}\")\n",
        "    return s\n",
        "\n",
        "SYSTEM_MSG = \"You are a careful logician. Follow the user's output format exactly.\"\n",
        "\n",
        "def build_user_text(premises, conclusion):\n",
        "    # premises can be list[str] or a single string; handle both\n",
        "    if isinstance(premises, (list, tuple)):\n",
        "        prem = \"\\n\".join([f\"- {p}\" for p in premises])\n",
        "    else:\n",
        "        prem = f\"- {premises}\"\n",
        "    return (\n",
        "        \"Task: Determine whether the conclusion is entailed, contradicted, or unknown given the premises.\\n\"\n",
        "        \"Premises:\\n\"\n",
        "        f\"{prem}\\n\\n\"\n",
        "        \"Conclusion:\\n\"\n",
        "        f\"{conclusion}\\n\\n\"\n",
        "        \"Output format: Answer: A (entailed), B (contradicted), or C (unknown).\\n\"\n",
        "        \"Answer:\"\n",
        "    )\n",
        "\n",
        "def map_ex(ex):\n",
        "    label = normalize_label(ex[\"label\"])\n",
        "    return {\n",
        "        \"user_text\": build_user_text(ex[\"premises\"], ex[\"conclusion\"]),\n",
        "        \"label\": label,\n",
        "        \"class_id\": [\"A\",\"B\",\"C\"].index(LABEL_TO_LETTER[label]),\n",
        "    }\n",
        "\n",
        "ds2 = DatasetDict({k: ds[k].map(map_ex, remove_columns=ds[k].column_names) for k in ds})\n",
        "print(\"Val label dist:\", Counter(ds2[\"validation\"][\"label\"]))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "models = [\n",
        "    \"Qwen/Qwen2.5-1.5B-Instruct\",\n",
        "    \"meta-llama/Llama-3.2-1B-Instruct\",\n",
        "    \"google/gemma-2-2b-it\",\n",
        "    \"stabilityai/stablelm-2-zephyr-1_6b\",\n",
        "    \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
        "]\n",
        "\n",
        "for m in models:\n",
        "    try:\n",
        "        AutoTokenizer.from_pretrained(m)\n",
        "        AutoModelForCausalLM.from_pretrained(m)\n",
        "        print(f\"✅ Access OK: {m}\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Access FAILED: {m}\\n{e}\\n\")\n"
      ],
      "metadata": {
        "id": "46SSVnI1SI4e"
      },
      "id": "46SSVnI1SI4e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0cf4b6a1",
      "metadata": {
        "id": "0cf4b6a1"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from datasets import DatasetDict\n",
        "from transformers import (\n",
        "    AutoTokenizer, AutoModelForCausalLM,\n",
        "    TrainingArguments, Trainer, DataCollatorWithPadding,\n",
        "    TrainerCallback, EarlyStoppingCallback\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "\n",
        "# -------------------------\n",
        "# Helpers: label token ids\n",
        "# -------------------------\n",
        "LABEL_CANDIDATES = [\" A\", \" B\", \" C\", \"A\", \"B\", \"C\"]  # prefer space-prefixed first\n",
        "\n",
        "def pick_label_token_ids(tokenizer):\n",
        "    ids = []\n",
        "    used = set()\n",
        "    for t in LABEL_CANDIDATES:\n",
        "        tok = tokenizer(t, add_special_tokens=False)[\"input_ids\"]\n",
        "        if len(tok) == 1 and tok[0] not in used:\n",
        "            ids.append(tok[0])\n",
        "            used.add(tok[0])\n",
        "    if len(ids) < 3:\n",
        "        raise ValueError(\"Could not find 3 single-token label candidates. Try adjusting LABEL_CANDIDATES.\")\n",
        "    # Map to A/B/C in order by matching the string form; we ensure first three unique tokens correspond to A,B,C variants.\n",
        "    # We'll explicitly re-pick in A,B,C order to be safe:\n",
        "    def one_id(s):\n",
        "        tok = tokenizer(s, add_special_tokens=False)[\"input_ids\"]\n",
        "        if len(tok) != 1:\n",
        "            return None\n",
        "        return tok[0]\n",
        "    for variant in [(\" A\",\"A\"), (\" B\",\"B\"), (\" C\",\"C\")]:\n",
        "        pass\n",
        "    A = one_id(\" A\") or one_id(\"A\")\n",
        "    B = one_id(\" B\") or one_id(\"B\")\n",
        "    C = one_id(\" C\") or one_id(\"C\")\n",
        "    if A is None or B is None or C is None:\n",
        "        raise ValueError(\"Could not locate single-token ids for A/B/C.\")\n",
        "    return [A, B, C]\n",
        "\n",
        "# -------------------------\n",
        "# Restricted-logit trainer\n",
        "# -------------------------\n",
        "class ContrastiveLabelTrainer(Trainer):\n",
        "    def __init__(self, *args, label_token_ids=None, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.label_token_ids = torch.tensor(label_token_ids, dtype=torch.long)\n",
        "\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None, **kwargs):\n",
        "        # inputs: input_ids, attention_mask, class_id\n",
        "        class_id = inputs.pop(\"class_id\")\n",
        "        label_ids = self.label_token_ids.to(model.device)  # [C]\n",
        "\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits  # [B, T, V]\n",
        "\n",
        "        # take logits at last non-pad token\n",
        "        attn = inputs[\"attention_mask\"]\n",
        "        last_idx = attn.sum(dim=1) - 1  # [B]\n",
        "        batch = torch.arange(logits.size(0), device=logits.device)\n",
        "        last_logits = logits[batch, last_idx]  # [B, V]\n",
        "\n",
        "        restricted = last_logits[:, label_ids]  # [B, C]\n",
        "        loss = torch.nn.functional.cross_entropy(restricted, class_id.to(model.device))\n",
        "        return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "def accuracy_from_restricted(restricted_logits, class_ids):\n",
        "    preds = restricted_logits.argmax(axis=-1)\n",
        "    return (preds == class_ids).mean().item()\n",
        "\n",
        "class ContrastiveEvalTrainer(ContrastiveLabelTrainer):\n",
        "    def prediction_step(self, model, inputs, prediction_loss_only, ignore_keys=None):\n",
        "        # We want to return restricted logits and class_id for metrics\n",
        "        with torch.no_grad():\n",
        "            loss = self.compute_loss(model, dict(inputs), return_outputs=False)\n",
        "            class_id = inputs[\"class_id\"].detach().cpu()\n",
        "            label_ids = self.label_token_ids.to(model.device)\n",
        "\n",
        "            outputs = model(\n",
        "                input_ids=inputs[\"input_ids\"],\n",
        "                attention_mask=inputs[\"attention_mask\"],\n",
        "            )\n",
        "            logits = outputs.logits  # [B,T,V]\n",
        "            attn = inputs[\"attention_mask\"]\n",
        "            last_idx = attn.sum(dim=1) - 1\n",
        "            batch = torch.arange(logits.size(0), device=logits.device)\n",
        "            last_logits = logits[batch, last_idx]  # [B,V]\n",
        "            restricted = last_logits[:, label_ids].detach().cpu()  # [B,C]\n",
        "\n",
        "        return (loss.detach().cpu(), restricted, class_id)\n",
        "\n",
        "def compute_metrics_from_restricted(eval_pred):\n",
        "    restricted_logits, class_ids = eval_pred\n",
        "    return {\"accuracy\": accuracy_from_restricted(restricted_logits, class_ids)}\n",
        "\n",
        "class PrintEval(TrainerCallback):\n",
        "    def on_evaluate(self, args, state, control, metrics=None, **kwargs):\n",
        "        if metrics:\n",
        "            print(f\"[EVAL step {state.global_step}] loss={metrics.get('eval_loss'):.4f} acc={metrics.get('eval_accuracy'):.4f}\")\n",
        "\n",
        "# -------------------------\n",
        "# LoRA target module finder\n",
        "# -------------------------\n",
        "CANDIDATE_TARGETS = [\n",
        "    \"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\n",
        "    \"gate_proj\",\"up_proj\",\"down_proj\",\n",
        "    \"Wqkv\",\"wo\",\"wq\",\"wk\",\"wv\",\n",
        "]\n",
        "\n",
        "def find_lora_targets(model):\n",
        "    names = set()\n",
        "    for n, _ in model.named_modules():\n",
        "        base = n.split(\".\")[-1]\n",
        "        if base in CANDIDATE_TARGETS:\n",
        "            names.add(base)\n",
        "    # prefer the common set if present\n",
        "    preferred = [x for x in [\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"] if x in names]\n",
        "    if preferred:\n",
        "        return preferred\n",
        "    # fallback: anything we found\n",
        "    return sorted(list(names))[:8] if names else [\"q_proj\",\"v_proj\"]\n",
        "\n",
        "# -------------------------\n",
        "# Main: loop models\n",
        "# -------------------------\n",
        "results = []\n",
        "\n",
        "for MODEL_NAME in MODEL_LIST:\n",
        "    print(\"\\n\" + \"=\"*90)\n",
        "    print(\"MODEL:\", MODEL_NAME)\n",
        "\n",
        "    # per-model output dir\n",
        "    safe = re.sub(r\"[^a-zA-Z0-9_.-]+\", \"_\", MODEL_NAME)\n",
        "    OUT_DIR = os.path.join(OUT_ROOT, safe)\n",
        "    os.makedirs(OUT_DIR, exist_ok=True)\n",
        "    print(\"OUT_DIR:\", OUT_DIR)\n",
        "\n",
        "    # tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    # tokenization uses chat template\n",
        "    def make_prompt_text(user_text: str) -> str:\n",
        "        # Prepend SYSTEM_MSG to the user_text as some models (like Gemma) often don't support a separate system role.\n",
        "        # This approach ensures the system instructions are still passed to the model.\n",
        "        full_user_text = f\"{SYSTEM_MSG}\\n\\n{user_text}\"\n",
        "        msgs = [\n",
        "            {\"role\":\"user\",\"content\": full_user_text},\n",
        "        ]\n",
        "        return tokenizer.apply_chat_template(msgs, tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "    label_token_ids = pick_label_token_ids(tokenizer)\n",
        "    print(\"Label token ids:\", label_token_ids, \"->\", [tokenizer.decode([i]) for i in label_token_ids])\n",
        "\n",
        "    def tokenize_ex(ex):\n",
        "        text = make_prompt_text(ex[\"user_text\"])\n",
        "        out = tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            max_length=MAX_LEN,\n",
        "            padding=False,\n",
        "        )\n",
        "        out[\"class_id\"] = ex[\"class_id\"]\n",
        "        return out\n",
        "\n",
        "    tok = DatasetDict({k: ds2[k].map(tokenize_ex, remove_columns=ds2[k].column_names) for k in ds2})\n",
        "\n",
        "    # model\n",
        "    torch_dtype = torch.bfloat16 if torch.cuda.is_available() else torch.float32\n",
        "    model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, torch_dtype=torch_dtype)\n",
        "\n",
        "    # LoRA\n",
        "    target_modules = find_lora_targets(model)\n",
        "    print(\"LoRA target_modules:\", target_modules)\n",
        "\n",
        "    lora_cfg = LoraConfig(\n",
        "        r=LORA_R,\n",
        "        lora_alpha=LORA_ALPHA,\n",
        "        lora_dropout=LORA_DROPOUT,\n",
        "        bias=\"none\",\n",
        "        task_type=\"CAUSAL_LM\",\n",
        "        target_modules=target_modules,\n",
        "    )\n",
        "    model = get_peft_model(model, lora_cfg)\n",
        "    model.print_trainable_parameters()\n",
        "\n",
        "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer, pad_to_multiple_of=8)\n",
        "\n",
        "    args = TrainingArguments(\n",
        "        output_dir=OUT_DIR,\n",
        "        seed=SEED,\n",
        "        num_train_epochs=EPOCHS,\n",
        "        per_device_train_batch_size=TRAIN_BS,\n",
        "        per_device_eval_batch_size=EVAL_BS,\n",
        "        gradient_accumulation_steps=GRAD_ACCUM,\n",
        "        learning_rate=LR,\n",
        "        weight_decay=WEIGHT_DECAY,\n",
        "        warmup_ratio=WARMUP_RATIO,\n",
        "        logging_steps=LOG_STEPS,\n",
        "        evaluation_strategy=\"steps\",\n",
        "        eval_steps=EVAL_STEPS,\n",
        "        save_strategy=\"steps\",\n",
        "        save_steps=SAVE_STEPS,\n",
        "        save_total_limit=2,\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"eval_loss\",\n",
        "        greater_is_better=False,\n",
        "        bf16=torch.cuda.is_available(),\n",
        "        fp16=False,\n",
        "        report_to=[],\n",
        "        remove_unused_columns=False,\n",
        "        label_names=[\"class_id\"],\n",
        "    )\n",
        "\n",
        "    trainer = ContrastiveEvalTrainer(\n",
        "        model=model,\n",
        "        args=args,\n",
        "        train_dataset=tok[\"train\"],\n",
        "        eval_dataset=tok[\"validation\"],\n",
        "        tokenizer=tokenizer,\n",
        "        data_collator=data_collator,\n",
        "        label_token_ids=label_token_ids,\n",
        "        compute_metrics=compute_metrics_from_restricted,\n",
        "        callbacks=[\n",
        "            PrintEval(),\n",
        "            EarlyStoppingCallback(\n",
        "                early_stopping_patience=EARLY_STOP_PATIENCE,\n",
        "                early_stopping_threshold=EARLY_STOP_THRESHOLD,\n",
        "            ),\n",
        "        ],\n",
        "    )\n",
        "\n",
        "    # Train\n",
        "    train_result = trainer.train()\n",
        "    print(train_result)\n",
        "\n",
        "    # Evaluate + save adapter\n",
        "    metrics = trainer.evaluate()\n",
        "    print(\"Final validation metrics:\", metrics)\n",
        "\n",
        "    model.save_pretrained(OUT_DIR)\n",
        "    tokenizer.save_pretrained(OUT_DIR)\n",
        "    print(\"Saved to:\", OUT_DIR)\n",
        "\n",
        "    # Update results table\n",
        "    row = {\n",
        "        \"model\": MODEL_NAME,\n",
        "        \"eval_loss\": float(metrics.get(\"eval_loss\", float(\"nan\"))),\n",
        "        \"eval_accuracy\": float(metrics.get(\"eval_accuracy\", float(\"nan\"))),\n",
        "        \"train_runtime_sec\": float(train_result.metrics.get(\"train_runtime\", float(\"nan\"))),\n",
        "    }\n",
        "    results.append(row)\n",
        "\n",
        "    df = pd.DataFrame(results).sort_values(\"eval_accuracy\", ascending=False)\n",
        "    print(\"\\n=== Results so far (sorted by eval_accuracy) ===\")\n",
        "    display(df)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "v99lBoBhkHHV"
      },
      "id": "v99lBoBhkHHV",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}