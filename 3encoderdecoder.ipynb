{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eneskosar/paper1/blob/main/3encoderdecoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "id": "f0112f1b",
      "cell_type": "code",
      "metadata": {
        "id": "f0112f1b"
      },
      "execution_count": null,
      "source": [
        "# CELL 0 — Mount Google Drive (for saving LoRA adapters & results)\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n"
      ],
      "outputs": []
    },
    {
      "id": "8f6a9d15",
      "cell_type": "code",
      "metadata": {
        "id": "8f6a9d15"
      },
      "execution_count": null,
      "source": [
        "# CELL 1 — Install dependencies (Colab)\n",
        "!pip -q install -U transformers datasets accelerate peft evaluate bitsandbytes\n"
      ],
      "outputs": []
    },
    {
      "id": "61cd7bef",
      "cell_type": "code",
      "metadata": {
        "id": "61cd7bef"
      },
      "execution_count": null,
      "source": [
        "# CELL 2 — (Required) Hugging Face login for gated FOLIO dataset\n",
        "# Run this BEFORE load_dataset(\"yale-nlp/FOLIO\")\n",
        "from huggingface_hub import login\n",
        "login()\n"
      ],
      "outputs": []
    },
    {
      "id": "729c650e",
      "cell_type": "code",
      "metadata": {
        "id": "729c650e"
      },
      "execution_count": null,
      "source": [
        "# CELL 3 — Imports + experiment config\n",
        "import os, gc, random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "from datasets import load_dataset, DatasetDict\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSeq2SeqLM,\n",
        "    DataCollatorForSeq2Seq,\n",
        "    Seq2SeqTrainingArguments,\n",
        "    Seq2SeqTrainer,\n",
        "    EarlyStoppingCallback,\n",
        "    TrainerCallback,\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "print(\"torch:\", torch.__version__)\n",
        "print(\"cuda available:\", torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print(\"gpu:\", torch.cuda.get_device_name(0))\n",
        "\n",
        "# ---- Encoder–Decoder model list (<2B) ----\n",
        "MODEL_LIST = [\n",
        "    \"google/flan-t5-base\",\n",
        "    \"google/flan-t5-large\",\n",
        "    \"facebook/bart-base\",\n",
        "    \"facebook/bart-large\",\n",
        "]\n",
        "\n",
        "# ---- Data / prompt ----\n",
        "MAX_SOURCE_LEN = 1024\n",
        "MAX_TARGET_LEN = 4   # output is just 'A'/'B'/'C'\n",
        "BATCH = 8\n",
        "\n",
        "# ---- Output ----\n",
        "OUT_ROOT = \"/content/drive/MyDrive/logic/folio_seq2seq_lora\"\n",
        "os.makedirs(OUT_ROOT, exist_ok=True)\n",
        "print(\"OUT_ROOT:\", OUT_ROOT)\n",
        "\n",
        "def cleanup():\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n"
      ],
      "outputs": []
    },
    {
      "id": "006c8523",
      "cell_type": "code",
      "metadata": {
        "id": "006c8523"
      },
      "execution_count": null,
      "source": [
        "# CELL 4 — Load FOLIO + build prompts (same format as your decoder notebook)\n",
        "from collections import Counter\n",
        "\n",
        "ds = load_dataset(\"yale-nlp/FOLIO\")\n",
        "print(ds)\n",
        "print(\"Train/Val sizes:\", len(ds[\"train\"]), len(ds[\"validation\"]))\n",
        "print(\"Columns:\", ds[\"train\"].column_names)\n",
        "\n",
        "LABEL_TO_LETTER = {\"True\":\"A\", \"False\":\"B\", \"Unknown\":\"C\"}\n",
        "ALT_LABELS = {\n",
        "    \"Uncertain\":\"Unknown\", \"uncertain\":\"Unknown\",\n",
        "    \"true\":\"True\", \"false\":\"False\", \"unknown\":\"Unknown\"\n",
        "}\n",
        "\n",
        "def normalize_label(lbl: str) -> str:\n",
        "    s = str(lbl).strip()\n",
        "    s = ALT_LABELS.get(s, s)\n",
        "    if s not in LABEL_TO_LETTER:\n",
        "        raise ValueError(f\"Unexpected label: {lbl!r}\")\n",
        "    return s\n",
        "\n",
        "def build_user_text(premises, conclusion):\n",
        "    # premises can be list[str] or a single string; handle both\n",
        "    if isinstance(premises, (list, tuple)):\n",
        "        prem = \"\\n\".join([f\"- {p}\" for p in premises])\n",
        "    else:\n",
        "        prem = f\"- {premises}\"\n",
        "    return (\n",
        "        \"Task: Determine whether the conclusion is entailed, contradicted, or unknown given the premises.\\n\"\n",
        "        \"Premises:\\n\"\n",
        "        f\"{prem}\\n\\n\"\n",
        "        \"Conclusion:\\n\"\n",
        "        f\"{conclusion}\\n\\n\"\n",
        "        \"Output format: Answer: A (entailed), B (contradicted), or C (unknown).\\n\"\n",
        "        \"Answer:\"\n",
        "    )\n",
        "\n",
        "def map_ex(ex):\n",
        "    label = normalize_label(ex[\"label\"])\n",
        "    return {\n",
        "        \"user_text\": build_user_text(ex[\"premises\"], ex[\"conclusion\"]),\n",
        "        \"label\": label,\n",
        "        \"label_letter\": LABEL_TO_LETTER[label],\n",
        "    }\n",
        "\n",
        "ds2 = DatasetDict({k: ds[k].map(map_ex, remove_columns=ds[k].column_names) for k in ds})\n",
        "print(\"Val label dist:\", Counter(ds2[\"validation\"][\"label\"]))\n",
        "print(\"\\n--- sample prompt ---\\n\")\n",
        "print(ds2[\"train\"][0][\"user_text\"])\n",
        "print(\"gold:\", ds2[\"train\"][0][\"label_letter\"])\n"
      ],
      "outputs": []
    },
    {
      "id": "442e5124",
      "cell_type": "code",
      "metadata": {
        "id": "442e5124"
      },
      "execution_count": null,
      "source": [
        "# CELL 5 — LoRA target selection (T5 vs BART) + metrics helpers\n",
        "\n",
        "def pick_lora_targets(model_name: str):\n",
        "    # T5 uses different module naming than BART\n",
        "    name = model_name.lower()\n",
        "    if \"t5\" in name:\n",
        "        # T5 attention: q, k, v, o (works for flan-t5)\n",
        "        return [\"q\", \"k\", \"v\", \"o\"]\n",
        "    else:\n",
        "        # BART attention projections\n",
        "        return [\"q_proj\", \"k_proj\", \"v_proj\", \"out_proj\"]\n",
        "\n",
        "def normalize_pred_letter(s: str) -> str:\n",
        "    if s is None:\n",
        "        return \"\"\n",
        "    s = s.strip()\n",
        "    if not s:\n",
        "        return \"\"\n",
        "    # take first non-space character\n",
        "    c = s[0].upper()\n",
        "    return c if c in {\"A\",\"B\",\"C\"} else \"\"\n",
        "\n",
        "def compute_accuracy(pred_texts, gold_texts):\n",
        "    preds = [normalize_pred_letter(t) for t in pred_texts]\n",
        "    golds = [normalize_pred_letter(t) for t in gold_texts]\n",
        "    invalid = sum(p == \"\" for p in preds)\n",
        "    acc = sum(p == g for p, g in zip(preds, golds)) / max(1, len(golds))\n",
        "    return acc, invalid / max(1, len(golds)), preds\n"
      ],
      "outputs": []
    },
    {
      "id": "213d2ce6",
      "cell_type": "code",
      "metadata": {
        "id": "213d2ce6"
      },
      "execution_count": null,
      "source": [
        "# CELL 6 — Tokenization for Seq2Seq (fixes eval_loss NaN) + custom callback for clean logging\n",
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass\n",
        "class RunningLog:\n",
        "    last_train_loss: float = float(\"nan\")\n",
        "\n",
        "running = RunningLog()\n",
        "\n",
        "class TableLoggerCallback(TrainerCallback):\n",
        "    \"\"\"Keeps the last seen training loss, and prints a compact table at eval steps.\"\"\"\n",
        "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
        "        if logs and \"loss\" in logs:\n",
        "            running.last_train_loss = float(logs[\"loss\"])\n",
        "\n",
        "def make_tokenize_fn(tokenizer):\n",
        "    def tokenize_batch(batch):\n",
        "        model_inputs = tokenizer(\n",
        "            batch[\"user_text\"],\n",
        "            max_length=MAX_SOURCE_LEN,\n",
        "            truncation=True,\n",
        "        )\n",
        "        # tokenize targets explicitly\n",
        "        with tokenizer.as_target_tokenizer():\n",
        "            lab = tokenizer(\n",
        "                batch[\"label_letter\"],\n",
        "                max_length=MAX_TARGET_LEN,\n",
        "                truncation=True,\n",
        "            )\n",
        "        labels = lab[\"input_ids\"]\n",
        "        # mask pad tokens to -100 so loss ignores them\n",
        "        pad_id = tokenizer.pad_token_id\n",
        "        labels = [[(t if t != pad_id else -100) for t in seq] for seq in labels]\n",
        "        model_inputs[\"labels\"] = labels\n",
        "        return model_inputs\n",
        "    return tokenize_batch\n",
        "\n",
        "def sanity_check_labels(tokenized_ds, tokenizer, split=\"validation\", n=50):\n",
        "    # ensure every example has at least one non -100 label token\n",
        "    bad = 0\n",
        "    for i in range(min(n, len(tokenized_ds[split]))):\n",
        "        labs = tokenized_ds[split][i][\"labels\"]\n",
        "        if sum(t != -100 for t in labs) == 0:\n",
        "            bad += 1\n",
        "    print(f\"Label sanity check ({split}, first {min(n, len(tokenized_ds[split]))}): bad={bad}\")\n"
      ],
      "outputs": []
    },
    {
      "id": "0fa71e26",
      "cell_type": "code",
      "metadata": {
        "id": "0fa71e26"
      },
      "execution_count": null,
      "source": [
        "# CELL 7 — Train/eval loop over encoder–decoder models (LoRA + EarlyStopping) + results table after each model\n",
        "RESULTS = []\n",
        "\n",
        "for model_name in MODEL_LIST:\n",
        "    print(\"\\n\" + \"=\"*100)\n",
        "    print(\"MODEL:\", model_name)\n",
        "\n",
        "    out_dir = os.path.join(OUT_ROOT, model_name.replace(\"/\", \"__\"))\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "    # tokenizer / model\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "    base = AutoModelForSeq2SeqLM.from_pretrained(\n",
        "        model_name,\n",
        "        torch_dtype=(torch.bfloat16 if torch.cuda.is_available() else None),\n",
        "        device_map=\"auto\" if torch.cuda.is_available() else None,\n",
        "    )\n",
        "\n",
        "    # LoRA\n",
        "    targets = pick_lora_targets(model_name)\n",
        "    lora_cfg = LoraConfig(\n",
        "        r=8,\n",
        "        lora_alpha=16,\n",
        "        lora_dropout=0.05,\n",
        "        bias=\"none\",\n",
        "        task_type=TaskType.SEQ_2_SEQ_LM,\n",
        "        target_modules=targets,\n",
        "    )\n",
        "    model = get_peft_model(base, lora_cfg)\n",
        "\n",
        "    # report trainable params\n",
        "    model.print_trainable_parameters()\n",
        "\n",
        "    # tokenize dataset for this tokenizer/model\n",
        "    tok_fn = make_tokenize_fn(tokenizer)\n",
        "    tokenized = ds2.map(tok_fn, batched=True, remove_columns=ds2[\"train\"].column_names)\n",
        "    sanity_check_labels(tokenized, tokenizer)\n",
        "\n",
        "    collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
        "\n",
        "    # accuracy from generation\n",
        "    def compute_metrics(eval_pred):\n",
        "        pred_ids, label_ids = eval_pred\n",
        "        # decode predictions\n",
        "        pred_texts = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
        "        # labels: replace -100 with pad to decode\n",
        "        label_ids = np.where(label_ids != -100, label_ids, tokenizer.pad_token_id)\n",
        "        gold_texts = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
        "\n",
        "        acc, invalid_rate, _ = compute_accuracy(pred_texts, gold_texts)\n",
        "        return {\"accuracy\": acc, \"invalid_rate\": invalid_rate}\n",
        "\n",
        "    from transformers import IntervalStrategy # Added import\n",
        "\n",
        "    # training args (bf16 preferred on A100; fall back otherwise)\n",
        "    use_bf16 = torch.cuda.is_available() and torch.cuda.get_device_capability(0)[0] >= 8\n",
        "    args = Seq2SeqTrainingArguments(\n",
        "        output_dir=out_dir,\n",
        "        per_device_train_batch_size=BATCH,\n",
        "        per_device_eval_batch_size=BATCH,\n",
        "        learning_rate=2e-4,\n",
        "        num_train_epochs=50,\n",
        "        warmup_ratio=0.03,\n",
        "        lr_scheduler_type=\"cosine\",\n",
        "        weight_decay=0.0,\n",
        "        eval_strategy=IntervalStrategy.STEPS, # Modified line\n",
        "        eval_steps=200,\n",
        "        save_steps=200,\n",
        "        logging_steps=50,\n",
        "        save_total_limit=2,\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"eval_accuracy\",\n",
        "        greater_is_better=True,\n",
        "        predict_with_generate=True,\n",
        "        generation_max_length=MAX_TARGET_LEN, # Changed from generation_max_new_tokens\n",
        "        generation_num_beams=1,\n",
        "        report_to=\"none\",\n",
        "        fp16=False,\n",
        "        bf16=bool(use_bf16),\n",
        "        dataloader_num_workers=2,\n",
        "        seed=SEED,\n",
        "    )\n",
        "\n",
        "    trainer = Seq2SeqTrainer(\n",
        "        model=model,\n",
        "        args=args,\n",
        "        train_dataset=tokenized[\"train\"],\n",
        "        eval_dataset=tokenized[\"validation\"],\n",
        "        tokenizer=tokenizer,\n",
        "        data_collator=collator,\n",
        "        compute_metrics=compute_metrics,\n",
        "        callbacks=[\n",
        "            TableLoggerCallback(),\n",
        "            EarlyStoppingCallback(early_stopping_patience=3),\n",
        "        ],\n",
        "    )\n",
        "\n",
        "    # train\n",
        "    train_toggle = trainer.train()\n",
        "    metrics = trainer.evaluate()\n",
        "\n",
        "    # grab best metrics\n",
        "    eval_acc = float(metrics.get(\"eval_accuracy\", float(\"nan\")))\n",
        "    eval_loss = float(metrics.get(\"eval_loss\", float(\"nan\")))\n",
        "    invalid = float(metrics.get(\"eval_invalid_rate\", float(\"nan\")))\n",
        "\n",
        "    # save LoRA adapter + tokenizer\n",
        "    trainer.model.save_pretrained(os.path.join(out_dir, \"lora_adapter\"))\n",
        "    tokenizer.save_pretrained(os.path.join(out_dir, \"tokenizer\"))\n",
        "\n",
        "    RESULTS.append({\n",
        "        \"model\": model_name,\n",
        "        \"trainable_params\": int(sum(p.numel() for p in model.parameters() if p.requires_grad)),\n",
        "        \"eval_accuracy\": eval_acc,\n",
        "        \"eval_loss\": eval_loss,\n",
        "        \"invalid_rate\": invalid,\n",
        "        \"best_checkpoint\": getattr(trainer.state, \"best_model_checkpoint\", None),\n",
        "    })\n",
        "\n",
        "    # print results table after each model\n",
        "    df = pd.DataFrame(RESULTS).sort_values(\"eval_accuracy\", ascending=False)\n",
        "    print(\"\\n--- RESULTS SO FAR ---\")\n",
        "    display(df)\n",
        "\n",
        "    # cleanup\n",
        "    del trainer, model, base, tokenizer, tokenized\n",
        "    cleanup()\n",
        "\n",
        "# final save\n",
        "final_df = pd.DataFrame(RESULTS).sort_values(\"eval_accuracy\", ascending=False)\n",
        "final_path = os.path.join(OUT_ROOT, \"results_seq2seq.csv\")\n",
        "final_df.to_csv(final_path, index=False)\n",
        "print(\"\\nSaved:\", final_path)\n",
        "display(final_df)"
      ],
      "outputs": []
    }
  ]
}