{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eneskosar/paper1/blob/main/1encoders_ft.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37ef62d4",
      "metadata": {
        "id": "37ef62d4"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1089ce1",
      "metadata": {
        "id": "d1089ce1"
      },
      "outputs": [],
      "source": [
        "!pip -q install -U \\\n",
        "  \"transformers>=4.41.0\" \\\n",
        "  \"datasets>=2.19.0\" \\\n",
        "  \"accelerate>=0.30.0\" \\\n",
        "  \"peft>=0.11.0\" \\\n",
        "  \"evaluate>=0.4.2\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a5f49abd",
      "metadata": {
        "id": "a5f49abd"
      },
      "source": [
        "# Multi-model experiment runner (no changes to training logic)\n",
        "This cell runs the same FoLiO LoRA fine-tuning pipeline across multiple **compatible** encoder checkpoints (DeBERTa family), and collects results in one table.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2cf797a",
      "metadata": {
        "id": "e2cf797a"
      },
      "outputs": [],
      "source": [
        "import os, gc\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorWithPadding,\n",
        "    set_seed,\n",
        ")\n",
        "import evaluate\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "# === Models to compare (small + large for each family) ===\n",
        "# Goal: try *different* pretrained backbones without changing the rest of your pipeline.\n",
        "# NOTE: different architectures have different module names, so LoRA `target_modules`\n",
        "# must be selected per-model (handled below).\n",
        "MODEL_LIST = [\n",
        "    # RoBERTa family (strong general encoder baseline)\n",
        "    \"roberta-base\",\n",
        "    \"roberta-large\",\n",
        "\n",
        "    # BERT family (classic baseline)\n",
        "    \"bert-base-uncased\",\n",
        "    \"bert-large-uncased\",\n",
        "\n",
        "    # ELECTRA family (often strong/faster discriminative encoder)\n",
        "    \"google/electra-small-discriminator\",\n",
        "    \"google/electra-large-discriminator\",\n",
        "\n",
        "    # DeBERTa-v3 family (very strong NLU / reasoning encoder)\n",
        "    \"microsoft/deberta-v3-base\",\n",
        "    \"microsoft/deberta-v3-large\",\n",
        "\n",
        "    # ALBERT family (parameter sharing; very large variant exists)\n",
        "    \"albert-base-v2\",\n",
        "    \"albert-xxlarge-v2\",\n",
        "\n",
        "    # XLM-RoBERTa family (different backbone; strong transformer encoder)\n",
        "    \"xlm-roberta-base\",\n",
        "    \"xlm-roberta-large\",\n",
        "]\n",
        "\n",
        "# Root folder for per-model outputs\n",
        "OUT_ROOT = \"/content/drive/MyDrive/folio_lora_experiments\"\n",
        "os.makedirs(OUT_ROOT, exist_ok=True)\n",
        "print(\"OUT_ROOT:\", OUT_ROOT)\n",
        "\n",
        "# === Shared dataset load (do once) ===\n",
        "ds = load_dataset(\"tasksource/folio\")  # train + validation available\n",
        "label_list = sorted(set(ds[\"train\"][\"label\"]))\n",
        "label2id = {lab: i for i, lab in enumerate(label_list)}\n",
        "id2label = {i: lab for lab, i in label2id.items()}\n",
        "num_labels = len(label_list)\n",
        "\n",
        "print(\"Labels:\", label_list)\n",
        "\n",
        "# Metrics (kept as in your working code)\n",
        "acc = evaluate.load(\"accuracy\")\n",
        "f1  = evaluate.load(\"f1\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    preds = np.argmax(logits, axis=-1)\n",
        "    out = {}\n",
        "    out.update(acc.compute(predictions=preds, references=labels))\n",
        "    out.update(f1.compute(predictions=preds, references=labels, average=\"macro\"))\n",
        "    return out\n",
        "\n",
        "# === Core run function (same logic, parameterized by BASE_MODEL/OUT_DIR) ===\n",
        "MAX_LEN = 1024  # If you hit OOM or shape errors, drop to 512/384.\n",
        "\n",
        "# Global training parameters for FLAN-T5 and LLaMA runs\n",
        "TRAIN_BSZ = 8\n",
        "EVAL_BSZ = 16\n",
        "LR = 2e-4\n",
        "EPOCHS = 30\n",
        "WD = 0.0\n",
        "WARMUP_RATIO = 0.06\n",
        "LOGGING_STEPS = 25\n",
        "EVAL_STRATEGY = \"steps\"\n",
        "EVAL_STEPS = 100\n",
        "SAVE_STRATEGY = \"steps\"\n",
        "SAVE_STEPS = 200 # For Llama/T5, let's keep it to a reasonable number, or 1 to save storage.\n",
        "SAVE_TOTAL_LIMIT = 1\n",
        "LOAD_BEST_AT_END = True\n",
        "BEST_METRIC = \"f1\" # Will be overridden for FLAN-T5\n",
        "REPORT_TO = \"none\"\n",
        "FP16 = True\n",
        "\n",
        "# Global LoRA parameters\n",
        "LORA_R = 8\n",
        "LORA_ALPHA = 16\n",
        "LORA_DROPOUT = 0.05\n",
        "\n",
        "# Define train_ds and dev_ds globally for run_one_llama and run_one_flan_t5\n",
        "train_ds = ds[\"train\"]\n",
        "dev_ds = ds[\"validation\"]\n",
        "\n",
        "def cleanup():\n",
        "    \"\"\"Frees VRAM and clears objects after each model run.\"\"\"\n",
        "    # Free VRAM between models\n",
        "    try:\n",
        "        # These variables might not be defined if an error occurred early\n",
        "        del trainer\n",
        "    except NameError: # Catch NameError specifically if trainer was never created\n",
        "        pass\n",
        "    except Exception: # Catch other exceptions during deletion\n",
        "        pass\n",
        "    try:\n",
        "        del model\n",
        "    except NameError:\n",
        "        pass\n",
        "    except Exception:\n",
        "        pass\n",
        "    try:\n",
        "        del base\n",
        "    except NameError:\n",
        "        pass\n",
        "    except Exception:\n",
        "        pass\n",
        "    gc.collect()\n",
        "    try:\n",
        "        import torch\n",
        "        torch.cuda.empty_cache()\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "def run_one(BASE_MODEL: str):\n",
        "    OUT_DIR = os.path.join(\n",
        "        OUT_ROOT,\n",
        "        BASE_MODEL.replace(\"/\", \"__\")\n",
        "    )\n",
        "    os.makedirs(OUT_DIR, exist_ok=True)\n",
        "    print(\"\\n\" + \"=\"*90)\n",
        "    print(\"MODEL:\", BASE_MODEL)\n",
        "    print(\"Saving to:\", OUT_DIR)\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True)\n",
        "\n",
        "    def preprocess(examples):\n",
        "        tok = tokenizer(\n",
        "            examples[\"premises\"],\n",
        "            examples[\"conclusion\"],\n",
        "            truncation=True,\n",
        "            max_length=MAX_LEN,\n",
        "        )\n",
        "        tok[\"labels\"] = [label2id[x] for x in examples[\"label\"]]\n",
        "        return tok\n",
        "\n",
        "    tokenized = ds.map(preprocess, batched=True, remove_columns=ds[\"train\"].column_names)\n",
        "\n",
        "    base = AutoModelForSequenceClassification.from_pretrained(\n",
        "        BASE_MODEL,\n",
        "        num_labels=num_labels,\n",
        "        id2label=id2label,\n",
        "        label2id=label2id,\n",
        "    )\n",
        "\n",
        "    # (kept) layer selection logic (not used in config below, but harmless)\n",
        "    num_layers = getattr(base.config, \"num_hidden_layers\", None)\n",
        "    if num_layers is not None:\n",
        "        start = int(num_layers * 3 / 4)\n",
        "        layers_to_lora = list(range(start, num_layers))\n",
        "    else:\n",
        "        layers_to_lora = None\n",
        "\n",
        "\n",
        "    def _pick_lora_targets(model):\n",
        "        \"\"\"Return LoRA target module name patterns that match this model family.\"\"\"\n",
        "        mt = getattr(getattr(model, \"config\", None), \"model_type\", None)\n",
        "\n",
        "        # DeBERTa (kept here in case you add it back later)\n",
        "        if mt in {\"deberta\", \"deberta-v2\", \"deberta-v3\"}:\n",
        "            return [\"query_proj\", \"key_proj\", \"value_proj\", \"intermediate.dense\", \"output.dense\"]\n",
        "\n",
        "        # RoBERTa / BERT (encoder-only, very similar internal naming)\n",
        "        if mt in {\"roberta\", \"bert\", \"albert\", \"xlm-roberta\"}:\n",
        "            return [\"query\", \"key\", \"value\", \"intermediate.dense\", \"output.dense\"]\n",
        "\n",
        "        # ELECTRA (BERT-like module naming)\n",
        "        if mt == \"electra\":\n",
        "            return [\"query\", \"key\", \"value\", \"intermediate.dense\", \"output.dense\"]\n",
        "\n",
        "        # Fallback: minimal, widely-matching attention projections (safer than 'dense' wildcard)\n",
        "        return [\"query\", \"key\", \"value\"]\n",
        "\n",
        "# LoRA config (completed; same intent as your existing cell)\n",
        "    lora_config = LoraConfig(\n",
        "        task_type=TaskType.SEQ_CLS,\n",
        "        r=8,\n",
        "        lora_alpha=16,\n",
        "        lora_dropout=0.05,\n",
        "        bias=\"none\",\n",
        "        # Explicitly define target modules for DeBERTa-family encoder blocks\n",
        "        target_modules=_pick_lora_targets(base),\n",
        "    )\n",
        "\n",
        "    model = get_peft_model(base, lora_config)\n",
        "    model.print_trainable_parameters()\n",
        "\n",
        "    data_collator = DataCollatorWithPadding(tokenizer)\n",
        "\n",
        "    args = TrainingArguments(\n",
        "        output_dir=OUT_DIR,\n",
        "        per_device_train_batch_size=8,\n",
        "        per_device_eval_batch_size=16,\n",
        "        gradient_accumulation_steps=1,\n",
        "        learning_rate=2e-4,\n",
        "        num_train_epochs=30,\n",
        "        max_grad_norm=1.0,\n",
        "        weight_decay=0.0,\n",
        "        warmup_ratio=0.06,\n",
        "        eval_strategy=\"steps\",\n",
        "        eval_steps=100,\n",
        "        logging_strategy=\"steps\",\n",
        "        logging_steps=25,\n",
        "        save_strategy=\"steps\",\n",
        "        save_steps=200,\n",
        "        save_total_limit=2,\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"f1\",\n",
        "        greater_is_better=True,\n",
        "        fp16=True,\n",
        "        report_to=\"none\",\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=args,\n",
        "        train_dataset=tokenized[\"train\"],\n",
        "        eval_dataset=tokenized[\"validation\"],\n",
        "        tokenizer=tokenizer,\n",
        "        data_collator=data_collator,\n",
        "        compute_metrics=compute_metrics,\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        trainer.train()\n",
        "        eval_out = trainer.evaluate()\n",
        "\n",
        "        # Save LoRA adapter + tokenizer\n",
        "        trainer.model.save_pretrained(OUT_DIR)\n",
        "        tokenizer.save_pretrained(OUT_DIR)\n",
        "\n",
        "        # Pull \"best\" info if available\n",
        "        best_metric = getattr(trainer.state, \"best_metric\", None)\n",
        "        best_ckpt   = getattr(trainer.state, \"best_model_checkpoint\", None)\n",
        "\n",
        "        row = {\n",
        "            \"model\": BASE_MODEL,\n",
        "            \"out_dir\": OUT_DIR,\n",
        "            \"eval_loss\": float(eval_out.get(\"eval_loss\", np.nan)),\n",
        "            \"eval_accuracy\": float(eval_out.get(\"eval_accuracy\", np.nan)),\n",
        "            \"eval_f1\": float(eval_out.get(\"eval_f1\", np.nan)),\n",
        "            \"best_metric\": float(best_metric) if best_metric is not None else np.nan,\n",
        "            \"best_checkpoint\": best_ckpt if best_ckpt is not None else \"\",\n",
        "        }\n",
        "        print(\"Final eval:\", row)\n",
        "        return row\n",
        "\n",
        "    except RuntimeError as e:\n",
        "        # Commonly OOM; keep the loop running\n",
        "        msg = str(e)\n",
        "        print(\"RuntimeError (likely OOM). Skipping this model.\\n\", msg[:600])\n",
        "        return {\n",
        "            \"model\": BASE_MODEL,\n",
        "            \"out_dir\": OUT_DIR,\n",
        "            \"eval_loss\": np.nan,\n",
        "            \"eval_accuracy\": np.nan,\n",
        "            \"eval_f1\": np.nan,\n",
        "            \"best_metric\": np.nan,\n",
        "            \"best_checkpoint\": \"\",\n",
        "            \"error\": msg[:300],\n",
        "        }\n",
        "    finally:\n",
        "        # Free VRAM between models\n",
        "        try:\n",
        "            del trainer\n",
        "        except Exception:\n",
        "            pass\n",
        "        try:\n",
        "            del model\n",
        "        except Exception:\n",
        "            pass\n",
        "        try:\n",
        "            del base\n",
        "        except Exception:\n",
        "            pass\n",
        "        gc.collect()\n",
        "        try:\n",
        "            import torch\n",
        "            torch.cuda.empty_cache()\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "# === Run the sweep ===\n",
        "results = []\n",
        "for m in MODEL_LIST:\n",
        "    results.append(run_one(m))\n",
        "\n",
        "\n",
        "# === Optional: add FLAN-T5 (Seq2Seq) and LLaMA-style (decoder) models ===\n",
        "# Notes:\n",
        "# - FLAN-T5 is encoder-decoder (text-to-text). It needs a separate Seq2SeqTrainer path.\n",
        "# - LLaMA-style models can be run as sequence classifiers via AutoModelForSequenceClassification,\n",
        "#   but Meta's official Llama checkpoints on HF are often *gated* (you may need to accept the license\n",
        "#   and use an HF token). We catch and record failures so the sweep continues.\n",
        "\n",
        "FLAN_T5_LIST = []  # removed (use encoder-only models)\n",
        "\n",
        "# Prefer Meta Llama if you have access; otherwise TinyLlama is a good \"LLaMA-like\" drop-in for experimentation.\n",
        "LLAMA_LIST = []  # removed (use encoder-only models)\n",
        "\n",
        "def run_one_llama(model_name: str):\n",
        "    # Same pipeline as encoders, but with LLaMA LoRA targets + padding token fix if needed.\n",
        "    import time\n",
        "    t0 = time.time()\n",
        "    out_dir = os.path.join(OUT_ROOT, model_name.replace(\"/\", \"__\")) # Fixed: Use OUT_ROOT and replace '/' with '__'\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "    try:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "        # LLaMA tokenizers often have no pad token by default\n",
        "        if tokenizer.pad_token is None:\n",
        "            tokenizer.pad_token = tokenizer.eos_token\n",
        "        tokenizer.padding_side = \"right\"\n",
        "\n",
        "        model = AutoModelForSequenceClassification.from_pretrained(\n",
        "            model_name,\n",
        "            num_labels=len(label_list),\n",
        "            id2label=id2label,\n",
        "            label2id=label2id,\n",
        "        )\n",
        "\n",
        "        # LoRA targets for LLaMA / decoder-only attention proj names\n",
        "        lora_cfg = LoraConfig(\n",
        "            task_type=TaskType.SEQ_CLS,\n",
        "            r=LORA_R,\n",
        "            lora_alpha=LORA_ALPHA,\n",
        "            lora_dropout=LORA_DROPOUT,\n",
        "            target_modules=[\"q_proj\", \"v_proj\"],\n",
        "        )\n",
        "        model = get_peft_model(model, lora_cfg)\n",
        "\n",
        "        def preprocess(examples):\n",
        "            tok = tokenizer(\n",
        "                examples[\"premises\"],\n",
        "                examples[\"conclusion\"],\n",
        "                truncation=True,\n",
        "                max_length=MAX_LEN,\n",
        "            )\n",
        "            tok[\"labels\"] = [label2id[x] for x in examples[\"label\"]]\n",
        "            return tok\n",
        "\n",
        "        tok_train = train_ds.map(preprocess, batched=True, remove_columns=train_ds.column_names)\n",
        "        tok_dev    = dev_ds.map(preprocess, batched=True, remove_columns=dev_ds.column_names)\n",
        "\n",
        "        collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "        args = TrainingArguments(\n",
        "            output_dir=out_dir,\n",
        "            per_device_train_batch_size=TRAIN_BSZ,\n",
        "            per_device_eval_batch_size=EVAL_BSZ,\n",
        "            learning_rate=LR,\n",
        "            num_train_epochs=EPOCHS,\n",
        "            weight_decay=WD,\n",
        "            warmup_ratio=WARMUP_RATIO,\n",
        "            logging_steps=LOGGING_STEPS,\n",
        "            evaluation_strategy=EVAL_STRATEGY,\n",
        "            eval_steps=EVAL_STEPS,\n",
        "            save_strategy=SAVE_STRATEGY,\n",
        "            save_steps=SAVE_STEPS,\n",
        "            save_total_limit=SAVE_TOTAL_LIMIT,\n",
        "            load_best_model_at_end=LOAD_BEST_AT_END,\n",
        "            metric_for_best_model=BEST_METRIC,\n",
        "            greater_is_better=True,\n",
        "            report_to=REPORT_TO,\n",
        "            fp16=FP16,\n",
        "        )\n",
        "\n",
        "        trainer = Trainer(\n",
        "            model=model,\n",
        "            args=args,\n",
        "            train_dataset=tok_train,\n",
        "            eval_dataset=tok_dev,\n",
        "            tokenizer=tokenizer,\n",
        "            data_collator=collator,\n",
        "            compute_metrics=compute_metrics,\n",
        "        )\n",
        "\n",
        "        trainer.train()\n",
        "        metrics = trainer.evaluate()\n",
        "        elapsed = time.time() - t0\n",
        "\n",
        "        return {\n",
        "            \"model\": model_name,\n",
        "            \"family\": \"llama_seqcls\",\n",
        "            \"status\": \"ok\",\n",
        "            \"eval_accuracy\": float(metrics.get(\"eval_accuracy\", np.nan)),\n",
        "            \"eval_loss\": float(metrics.get(\"eval_loss\", np.nan)),\n",
        "            \"seconds\": elapsed,\n",
        "            \"out_dir\": out_dir,\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        elapsed = time.time() - t0\n",
        "        return {\n",
        "            \"model\": model_name,\n",
        "            \"family\": \"llama_seqcls\",\n",
        "            \"status\": f\"failed: {type(e).__name__}\",\n",
        "            \"eval_accuracy\": np.nan,\n",
        "            \"eval_loss\": np.nan,\n",
        "            \"seconds\": elapsed,\n",
        "            \"out_dir\": out_dir,\n",
        "            \"error\": str(e)[:300],\n",
        "        }\n",
        "    finally:\n",
        "        cleanup()\n",
        "\n",
        "def run_one_flan_t5(model_name: str):\n",
        "    # Text-to-text fine-tuning: generate the label string.\n",
        "    import time\n",
        "    from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq\n",
        "\n",
        "    t0 = time.time()\n",
        "    out_dir = os.path.join(OUT_ROOT, model_name.replace(\"/\", \"__\")) # Fixed: Use OUT_ROOT and replace '/' with '__'\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "    try:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "        model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "        # Prompt format (kept simple and deterministic)\n",
        "        def preprocess(examples):\n",
        "            inputs = [\n",
        "                f\"premise: {p} conclusion: {c} label:\"\n",
        "                for p, c in zip(examples[\"premises\"], examples[\"conclusion\"])\n",
        "            ]\n",
        "            targets = [str(x) for x in examples[\"label\"]]\n",
        "\n",
        "            model_inputs = tokenizer(\n",
        "                inputs,\n",
        "                truncation=True,\n",
        "                max_length=MAX_LEN,\n",
        "            )\n",
        "            with tokenizer.as_target_tokenizer():\n",
        "                labels = tokenizer(\n",
        "                    targets,\n",
        "                    truncation=True,\n",
        "                    max_length=16,\n",
        "                )\n",
        "            model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "            return model_inputs\n",
        "\n",
        "        tok_train = train_ds.map(preprocess, batched=True, remove_columns=train_ds.column_names)\n",
        "        tok_dev   = dev_ds.map(preprocess, batched=True, remove_columns=dev_ds.column_names)\n",
        "\n",
        "        data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
        "\n",
        "        def compute_metrics_t5(eval_pred):\n",
        "            preds, label_ids = eval_pred\n",
        "            # Some trainer versions return a tuple\n",
        "            if isinstance(preds, tuple):\n",
        "                preds = preds[0]\n",
        "            pred_str = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "            label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
        "\n",
        "            pred_str = [p.strip() for p in pred_str]\n",
        "            label_str = [l.strip() for l in label_str]\n",
        "\n",
        "            acc = sum(int(p == l) for p, l in zip(pred_str, label_str)) / max(1, len(label_str))\n",
        "            return {\"accuracy\": acc}\n",
        "\n",
        "        args = Seq2SeqTrainingArguments(\n",
        "            output_dir=out_dir,\n",
        "            per_device_train_batch_size=TRAIN_BSZ,\n",
        "            per_device_eval_batch_size=EVAL_BSZ,\n",
        "            learning_rate=LR,\n",
        "            num_train_epochs=EPOCHS,\n",
        "            weight_decay=WD,\n",
        "            warmup_ratio=WARMUP_RATIO,\n",
        "            logging_steps=LOGGING_STEPS,\n",
        "            # Removed evaluation_strategy as it caused a TypeError\n",
        "            eval_steps=EVAL_STEPS,\n",
        "            save_strategy=SAVE_STRATEGY,\n",
        "            save_steps=SAVE_STEPS,\n",
        "            save_total_limit=SAVE_TOTAL_LIMIT,\n",
        "            load_best_model_at_end=LOAD_BEST_AT_END,\n",
        "            metric_for_best_model=\"accuracy\", # Fixed: Set to accuracy for Seq2SeqTrainer\n",
        "            greater_is_better=True,\n",
        "            report_to=REPORT_TO,\n",
        "            fp16=FP16,\n",
        "            predict_with_generate=True,\n",
        "            generation_max_length=16,\n",
        "        )\n",
        "\n",
        "        trainer = Seq2SeqTrainer(\n",
        "            model=model,\n",
        "            args=args,\n",
        "            train_dataset=tok_train,\n",
        "            eval_dataset=tok_dev,\n",
        "            tokenizer=tokenizer,\n",
        "            data_collator=data_collator,\n",
        "            compute_metrics=compute_metrics_t5,\n",
        "        )\n",
        "\n",
        "        trainer.train()\n",
        "        metrics = trainer.evaluate()\n",
        "        elapsed = time.time() - t0\n",
        "\n",
        "        return {\n",
        "            \"model\": model_name,\n",
        "            \"family\": \"flan_t5_seq2seq\",\n",
        "            \"status\": \"ok\",\n",
        "            \"eval_accuracy\": float(metrics.get(\"eval_accuracy\", metrics.get(\"eval_accuracy\", np.nan))),\n",
        "            \"eval_loss\": float(metrics.get(\"eval_loss\", np.nan)),\n",
        "            \"seconds\": elapsed,\n",
        "            \"out_dir\": out_dir,\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        elapsed = time.time() - t0\n",
        "        return {\n",
        "            \"model\": model_name,\n",
        "            \"family\": \"flan_t5_seq2seq\",\n",
        "            \"status\": f\"failed: {type(e).__name__}\",\n",
        "            \"eval_accuracy\": np.nan,\n",
        "            \"eval_loss\": np.nan,\n",
        "            \"seconds\": elapsed,\n",
        "            \"out_dir\": out_dir,\n",
        "            \"error\": str(e)[:300],\n",
        "        }\n",
        "    finally:\n",
        "        cleanup()\n",
        "\n",
        "# --- Run everything and collect results ---\n",
        "df_enc = pd.DataFrame(results)\n",
        "\n",
        "extra_results = []\n",
        "for m in FLAN_T5_LIST:\n",
        "    extra_results.append(run_one_flan_t5(m))\n",
        "\n",
        "for m in LLAMA_LIST:\n",
        "    extra_results.append(run_one_llama(m))\n",
        "\n",
        "df_extra = pd.DataFrame(extra_results)\n",
        "\n",
        "df_all = pd.concat([df_enc, df_extra], ignore_index=True)\n",
        "df_all\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Keep only the required columns in the final results table\n",
        "FINAL_COLS = [\n",
        "    \"model\",\n",
        "    \"eval_loss\",\n",
        "    \"eval_accuracy\",\n",
        "    \"eval_f1\",\n",
        "    \"best_metric\",\n",
        "]\n",
        "\n",
        "df_final = df_all[FINAL_COLS].copy()\n",
        "\n",
        "# Optional: sort by best metric (usually accuracy)\n",
        "df_final = df_final.sort_values(\n",
        "    by=\"best_metric\",\n",
        "    ascending=False\n",
        ").reset_index(drop=True)\n",
        "\n",
        "df_final\n"
      ],
      "metadata": {
        "id": "P3rMNNNIf3HR"
      },
      "id": "P3rMNNNIf3HR",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}